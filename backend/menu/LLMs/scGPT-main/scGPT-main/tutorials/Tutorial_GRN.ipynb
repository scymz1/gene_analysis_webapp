{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a1bd7ef",
   "metadata": {},
   "source": [
    "# GRN Inference on Pre-trained Model\n",
    "Here we use the pre-trained blood model as an example for GRN inference, particularly regarding gene program extraction and network visualization. We also present the cell-type specific activations within these gene programs on the Immune Human dataset, as a soft validation for the zero-shot performance. \n",
    "\n",
    "Note that GRN inference can be performed on pre-trained and finetuned models as showcased in our manuscript.\n",
    "\n",
    "Users may perform scGPT's gene-embedding-based GRN inference in the following steps:\n",
    "\n",
    "     1. Load optimized scGPT model (pre-trained or fine-tuned) and data\n",
    "     \n",
    "     2. Retrieve scGPT's gene embeddings\n",
    "     \n",
    "     3. Extract gene programs from scGPT's gene embedding network\n",
    "     \n",
    "     4. Visualize gene program activations on dataset of interest\n",
    "     \n",
    "     5. Visualize the interconnectivity of genes within select gene programs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b08bed",
   "metadata": {},
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import gseapy as gp\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.tasks import GeneEmbedding\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.utils import set_seed \n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948bbc49",
   "metadata": {},
   "source": [
    "set_seed(42)\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "n_hvg = 1200\n",
    "n_bins = 51\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = n_bins"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9565a71c",
   "metadata": {},
   "source": [
    "## Step 1: Load pre-trained model and dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83ad0952",
   "metadata": {},
   "source": [
    "### 1.1  Load pre-trained model\n",
    "The blood pre-trained model can be downloaded via this [link](https://drive.google.com/drive/folders/1kkug5C7NjvXIwQGGaGoqXTk_Lb_pDrBU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86247948",
   "metadata": {},
   "source": [
    "# Specify model path; here we load the pre-trained scGPT blood model\n",
    "model_dir = Path(\"../save/scGPT_bc\")\n",
    "model_config_file = model_dir / \"args.json\"\n",
    "model_file = model_dir / \"best_model.pt\"\n",
    "vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "for s in special_tokens:\n",
    "    if s not in vocab:\n",
    "        vocab.append_token(s)\n",
    "\n",
    "# Retrieve model parameters from config files\n",
    "with open(model_config_file, \"r\") as f:\n",
    "    model_configs = json.load(f)\n",
    "print(\n",
    "    f\"Resume model from {model_file}, the model args will override the \"\n",
    "    f\"config {model_config_file}.\"\n",
    ")\n",
    "embsize = model_configs[\"embsize\"]\n",
    "nhead = model_configs[\"nheads\"]\n",
    "d_hid = model_configs[\"d_hid\"]\n",
    "nlayers = model_configs[\"nlayers\"]\n",
    "n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "gene2idx = vocab.get_stoi()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19de9b6c",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    pad_value=pad_value,\n",
    "    n_input_bins=n_input_bins,\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    print(f\"Loading all model params from {model_file}\")\n",
    "except:\n",
    "    # only load params that are in the model and match the size\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(model_file)\n",
    "    pretrained_dict = {\n",
    "        k: v\n",
    "        for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    for k, v in pretrained_dict.items():\n",
    "        print(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5e23d36",
   "metadata": {},
   "source": [
    "### 1.2  Load dataset of interest\n",
    "The Immune Human dataset can be downloaded via this [link](https://figshare.com/ndownloader/files/25717328)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec95b62",
   "metadata": {},
   "source": [
    "# Specify data path; here we load the Immune Human dataset\n",
    "data_dir = Path(\"../data\")\n",
    "adata = sc.read(\n",
    "    str(data_dir / \"Immune_ALL_human.h5ad\"), cache=True\n",
    ")  # 33506 Ã— 12303\n",
    "ori_batch_col = \"batch\"\n",
    "adata.obs[\"celltype\"] = adata.obs[\"final_annotation\"].astype(str)\n",
    "data_is_raw = False"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f1a521e",
   "metadata": {},
   "source": [
    "# Preprocess the data following the scGPT data pre-processing pipeline\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=\"batch\")"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f752d133",
   "metadata": {},
   "source": [
    "## Step 2: Retrieve scGPT's gene embeddings\n",
    "\n",
    "Note that technically scGPT's gene embeddings are data independent. Overall, the pre-trained foundation model contains 30+K genes. Here for simplicity, we focus on a subset of HVGs specific to the data at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f190778",
   "metadata": {},
   "source": [
    "# Retrieve the data-independent gene embeddings from scGPT\n",
    "gene_ids = np.array([id for id in gene2idx.values()])\n",
    "gene_embeddings = model.encoder(torch.tensor(gene_ids, dtype=torch.long).to(device))\n",
    "gene_embeddings = gene_embeddings.detach().cpu().numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5efa8903",
   "metadata": {},
   "source": [
    "# Filter on the intersection between the Immune Human HVGs found in step 1.2 and scGPT's 30+K foundation model vocab\n",
    "gene_embeddings = {gene: gene_embeddings[i] for i, gene in enumerate(gene2idx.keys()) if gene in adata.var.index.tolist()}\n",
    "print('Retrieved gene embeddings for {} genes.'.format(len(gene_embeddings)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1181bcd5",
   "metadata": {},
   "source": [
    "# Construct gene embedding network\n",
    "embed = GeneEmbedding(gene_embeddings)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71b9e16d",
   "metadata": {},
   "source": [
    "## Step 3: Extract gene programs from gene embedding network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bc58b46",
   "metadata": {},
   "source": [
    "### 3.1  Perform Louvain clustering on the gene embedding network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "569ea02a",
   "metadata": {},
   "source": [
    "# Perform Louvain clustering with desired resolution; here we specify resolution=40\n",
    "gdata = embed.get_adata(resolution=40)\n",
    "# Retrieve the gene clusters\n",
    "metagenes = embed.get_metagenes(gdata)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38dad229",
   "metadata": {},
   "source": [
    "### 3.2  Filter on clusters with 5 or more genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c763acf3",
   "metadata": {},
   "source": [
    "# Obtain the set of gene programs from clusters with #genes >= 5\n",
    "mgs = dict()\n",
    "for mg, genes in metagenes.items():\n",
    "    if len(genes) > 4:\n",
    "        mgs[mg] = genes"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3671c64b",
   "metadata": {},
   "source": [
    "# Here are the gene programs identified\n",
    "mgs"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd6066cd",
   "metadata": {},
   "source": [
    "## Step 4: Visualize gene program activation on the Immune Human dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "078a3233",
   "metadata": {},
   "source": [
    "sns.set(font_scale=0.35)\n",
    "embed.score_metagenes(adata, metagenes)\n",
    "embed.plot_metagenes_scores(adata, mgs, \"celltype\")"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b3d918c",
   "metadata": {},
   "source": [
    "## Step 5: Visualize network connectivity within desired gene program\n",
    "We can further visualize the connectivity between genes within any gene program of interest from Step 4. Here is an example of gene program 3 consisting of the CD3 cluster, CD8 cluster and other genes. In the visualization, we see strong connections highlighted in blue (by cosine similarity) between CD3D, E, and G, as well as CD8A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d1a61b1",
   "metadata": {},
   "source": [
    "# Retrieve gene program 3 which contains the CD3 gene set\n",
    "CD_genes = mgs['3']\n",
    "print(CD_genes)\n",
    "# Compute cosine similarities among genes in this gene program\n",
    "df_CD = pd.DataFrame(columns=['Gene', 'Similarity', 'Gene1'])\n",
    "for i in tqdm.tqdm(CD_genes):\n",
    "    df = embed.compute_similarities(i, CD_genes)\n",
    "    df['Gene1'] = i\n",
    "    df_CD = df_CD.append(df)\n",
    "df_CD_sub = df_CD[df_CD['Similarity']<0.99].sort_values(by='Gene') # Filter out edges from each gene to itself"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7060ce1",
   "metadata": {},
   "source": [
    "# Creates a graph from the cosine similarity network\n",
    "input_node_weights = [(row['Gene'], row['Gene1'], round(row['Similarity'], 2)) for i, row in df_CD_sub.iterrows()]\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(input_node_weights)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff192716",
   "metadata": {},
   "source": [
    "# Plot the cosine similarity network; strong edges (> select threshold) are highlighted\n",
    "thresh = 0.4\n",
    "plt.figure(figsize=(20, 20))\n",
    "widths = nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > thresh]\n",
    "esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= thresh]\n",
    "\n",
    "pos = nx.spring_layout(G, k=0.4, iterations=15, seed=3)\n",
    "\n",
    "width_large = {}\n",
    "width_small = {}\n",
    "for i, v in enumerate(list(widths.values())):\n",
    "    if v > thresh:\n",
    "        width_large[list(widths.keys())[i]] = v*10\n",
    "    else:\n",
    "        width_small[list(widths.keys())[i]] = max(v, 0)*10\n",
    "\n",
    "nx.draw_networkx_edges(G, pos,\n",
    "                       edgelist = width_small.keys(),\n",
    "                       width=list(width_small.values()),\n",
    "                       edge_color='lightblue',\n",
    "                       alpha=0.8)\n",
    "nx.draw_networkx_edges(G, pos, \n",
    "                       edgelist = width_large.keys(), \n",
    "                       width = list(width_large.values()), \n",
    "                       alpha = 0.5, \n",
    "                       edge_color = \"blue\", \n",
    "                      )\n",
    "# node labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=25, font_family=\"sans-serif\")\n",
    "# edge weight labels\n",
    "d = nx.get_edge_attributes(G, \"weight\")\n",
    "edge_labels = {k: d[k] for k in elarge}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=15)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac302953",
   "metadata": {},
   "source": [
    "## Step 6: Reactome pathway analysis\n",
    "Again with gene program 3 as an example, users may perform pathway enrichment analysis to identify related pathways. In the paper, we used the Bonferroni correction to adjust the p-value threshold by accounting for the total number of tests performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3a07188",
   "metadata": {},
   "source": [
    "# Meta info about the number of terms (tests) in the databases\n",
    "df_database = pd.DataFrame(\n",
    "data = [['GO_Biological_Process_2021', 6036],\n",
    "['GO_Molecular_Function_2021', 1274],\n",
    "['Reactome_2022', 1818]],\n",
    "columns = ['dataset', 'term'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a3e18c9",
   "metadata": {},
   "source": [
    "# Select desired database for query; here use Reactome as an example\n",
    "databases = ['Reactome_2022']\n",
    "m = df_database[df_database['dataset'].isin(databases)]['term'].sum()\n",
    "# p-value correction for total number of tests done\n",
    "p_thresh = 0.05/m"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a1da728",
   "metadata": {},
   "source": [
    "# Perform pathway enrichment analysis using the gseapy package in the Reactome database\n",
    "df = pd.DataFrame()\n",
    "enr_Reactome = gp.enrichr(gene_list=CD_genes,\n",
    "                          gene_sets=databases,\n",
    "                          organism='Human', \n",
    "                          outdir='test/enr_Reactome',\n",
    "                          cutoff=0.5)\n",
    "out = enr_Reactome.results\n",
    "out = out[out['P-value'] < p_thresh]\n",
    "df = df.append(out, ignore_index=True)\n",
    "df"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
