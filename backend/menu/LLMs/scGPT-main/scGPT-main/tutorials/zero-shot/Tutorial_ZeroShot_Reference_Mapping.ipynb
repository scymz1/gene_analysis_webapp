{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot reference mapping tutorial with scGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial covers the zero-shot reference mapping with scGPT. This workflow achieves accurate and fast reference mapping for scRNA-seq datasets without fine-tuning (or any extensive training) of scGPT. To further boost the performance, we recommend fine-tuning scGPT.\n",
    "\n",
    "We will use COVID-19 dataset to demonstrate the zero-shot reference mapping. You can download the processed reference and query datasets from [here](https://drive.google.com/drive/folders/1jSPoPunGQOmd71vDsK0FS7UvmDhGdhQS?usp=sharing). The COVID-19 dataset is derived from the work by [Lotfollahi et al](https://www.nature.com/articles/s41587-021-01001-7), which contains 18 distinct batches and diverse samples from lung tissues. The reference dataset consists of 15,997 cells and the query dataset contains 4,003 cells. You may place the dataset under `data` directory in the outer level.\n",
    "\n",
    "Particularly, we use the `scGPT_human` model to provide embeddings out of the box. You may download it from [here](https://drive.google.com/drive/folders/1oWh_-ZRdhtoGQ2Fw24HP41FgLoomVo-y). \n",
    "\n",
    "\n",
    "The zero-shot reference mapping workflow is as follows:\n",
    "\n",
    " 1. Load and pre-process the dataset\n",
    "    \n",
    " 2. Generate scGPT embeddings for each cell in reference and query datasets\n",
    "\n",
    " 3. Transfer the annotations from reference to query dataset\n",
    "\n",
    "At the [appendix](#appendix-reference-mapping-on-lung-kim-dataset) of this tutorial, we will also showcase the zero-shot reference mapping on Lung dataset. You can find the dataset [here](https://drive.google.com/drive/folders/1gbfO7VqxCOkfzgHAih6hO88zFv6pd8wO?usp=sharing).\n",
    "\n",
    "\n",
    "We use a similarity-based method for transferring the annotation, which involves comparing the similarity between the query cell embedding and the reference cell embeddings. We use [FAISS](https://github.com/facebookresearch/faiss) to perform the similarity search.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bowang-lab/scGPT/blob/main/tutorials/zero-shot/Tutorial_ZeroShot_Reference_Mapping.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# Specifically for Google Colab, install dependencies and download data\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    print(\"Installing dependencies...\")\n",
    "    !pip install -U scgpt\n",
    "    # the optional dependency of flash-attion is skipped on colab\n",
    "    !pip install wandb louvain faiss-cpu\n",
    "\n",
    "    # NOTE: May need to restart runtime after the installation\n",
    "\n",
    "    print(\"Downloading data and model ckpt...\")\n",
    "    !pip install -q -U gdown\n",
    "    import gdown\n",
    "\n",
    "    data_dir = \"../../data\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "    if not os.path.exists(os.path.join(data_dir, \"covid\")):\n",
    "        gdown.download_folder(\n",
    "            \"https://drive.google.com/drive/folders/1jSPoPunGQOmd71vDsK0FS7UvmDhGdhQS\",\n",
    "            output=os.path.join(data_dir),\n",
    "        )\n",
    "    if not os.path.exists(os.path.join(data_dir, \"lung\")):\n",
    "        gdown.download_folder(\n",
    "            \"https://drive.google.com/drive/folders/1gbfO7VqxCOkfzgHAih6hO88zFv6pd8wO\",\n",
    "            output=os.path.join(data_dir),\n",
    "        )\n",
    "\n",
    "    print(\"Downloading model ckpt...\")\n",
    "    model_dir = \"../../save/scGPT_human\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        !mkdir -p $model_dir\n",
    "        gdown.download_folder(\n",
    "            \"https://drive.google.com/drive/folders/1oWh_-ZRdhtoGQ2Fw24HP41FgLoomVo-y\",\n",
    "            output=model_dir,\n",
    "        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import scGPT and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "import scanpy as sc\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import scgpt as scg\n",
    "\n",
    "# extra dependency for similarity search\n",
    "try:\n",
    "    import faiss\n",
    "\n",
    "    faiss_imported = True\n",
    "except ImportError:\n",
    "    faiss_imported = False\n",
    "    print(\n",
    "        \"faiss not installed! We highly recommend installing it for fast similarity search.\"\n",
    "    )\n",
    "    print(\"To install it, see https://github.com/facebookresearch/faiss/wiki/Installing-Faiss\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ResourceWarning)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset, you may download the dataset from [here](https://drive.google.com/drive/folders/1jSPoPunGQOmd71vDsK0FS7UvmDhGdhQS?usp=sharing). We set the columns storing gene name columns, batch key and cell type key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "model_dir = Path(\"../../save/scGPT_human\")\n",
    "adata = sc.read_h5ad('../../data/covid/batch_covid_subsampled_train.h5ad')\n",
    "test_adata = sc.read_h5ad('../../data/covid/batch_covid_subsampled_test.h5ad')\n",
    "\n",
    "cell_type_key = \"celltype\"\n",
    "gene_col = \"gene_name\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed the reference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "ref_embed_adata = scg.tasks.embed_data(\n",
    "    adata,\n",
    "    model_dir,\n",
    "    gene_col=gene_col,\n",
    "    batch_size=64,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed the query dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "test_embed_adata = scg.tasks.embed_data(\n",
    "    test_adata,\n",
    "    model_dir,\n",
    "    gene_col=gene_col,\n",
    "    batch_size=64,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark the reference vs. query cells and mask the cell types on query cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "# concatenate the two datasets\n",
    "adata_concat = test_embed_adata.concatenate(ref_embed_adata, batch_key=\"dataset\")\n",
    "# mark the reference vs. query dataset\n",
    "adata_concat.obs[\"is_ref\"] = [\"Query\"] * len(test_embed_adata) + [\"Reference\"] * len(\n",
    "    ref_embed_adata\n",
    ")\n",
    "adata_concat.obs[\"is_ref\"] = adata_concat.obs[\"is_ref\"].astype(\"category\")\n",
    "# mask the query dataset cell types\n",
    "adata_concat.obs[cell_type_key] = adata_concat.obs[cell_type_key].astype(\"category\")\n",
    "adata_concat.obs[cell_type_key] = adata_concat.obs[cell_type_key].cat.add_categories([\"To be predicted\"])\n",
    "adata_concat.obs[cell_type_key][: len(test_embed_adata)] = \"To be predicted\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the embeddings from query and reference datasets using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "sc.pp.neighbors(adata_concat, use_rep=\"X_scGPT\")\n",
    "sc.tl.umap(adata_concat)\n",
    "sc.pl.umap(\n",
    "    adata_concat, color=[\"is_ref\", cell_type_key], wspace=0.4, frameon=False, ncols=1\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference mapping and transfer the annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the reference mapping using cell-level majority voting. You may adjust the `k` parameter to control the number of nearest neighbors to consider for voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "# Those functions are only used when faiss is not installed\n",
    "def l2_sim(a, b):\n",
    "    sims = -np.linalg.norm(a - b, axis=1)\n",
    "    return sims\n",
    "\n",
    "def get_similar_vectors(vector, ref, top_k=10):\n",
    "        # sims = cos_sim(vector, ref)\n",
    "        sims = l2_sim(vector, ref)\n",
    "        \n",
    "        top_k_idx = np.argsort(sims)[::-1][:top_k]\n",
    "        return top_k_idx, sims[top_k_idx]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "ref_cell_embeddings = ref_embed_adata.obsm[\"X_scGPT\"]\n",
    "test_emebd = test_embed_adata.obsm[\"X_scGPT\"]\n",
    "\n",
    "k = 10  # number of neighbors\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(ref_cell_embeddings.shape[1])\n",
    "index.add(ref_cell_embeddings)\n",
    "\n",
    "# Query dataset, k - number of closest elements (returns 2 numpy arrays)\n",
    "distances, labels = index.search(test_emebd, k)\n",
    "\n",
    "idx_list=[i for i in range(test_emebd.shape[0])]\n",
    "preds = []\n",
    "sim_list = distances\n",
    "for k in idx_list:\n",
    "    if faiss_imported:\n",
    "        idx = labels[k]\n",
    "    else:\n",
    "        idx, sim = get_similar_vectors(test_emebd[k][np.newaxis, ...], ref_cell_embeddings, k)\n",
    "    pred = ref_embed_adata.obs[cell_type_key][idx].value_counts()\n",
    "    preds.append(pred.index[0])\n",
    "gt = test_adata.obs[cell_type_key].to_numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "res_dict = {\n",
    "    \"accuracy\": accuracy_score(gt, preds),\n",
    "    \"precision\": precision_score(gt, preds, average=\"macro\"),\n",
    "    \"recall\": recall_score(gt, preds, average=\"macro\"),\n",
    "    \"macro_f1\": f1_score(gt, preds, average=\"macro\"),\n",
    "}\n",
    "\n",
    "res_dict"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "y_true = gt\n",
    "y_pred = preds\n",
    "cell_type_list = np.unique(y_true)\n",
    "matrix = confusion_matrix(y_true, y_pred, labels=cell_type_list)\n",
    "matrix = matrix.astype(\"float\") / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "df = pd.DataFrame(matrix, index=cell_type_list[:matrix.shape[0]], columns=cell_type_list[:matrix.shape[1]])\n",
    "\n",
    "ax = sns.clustermap(df,  \n",
    "                    cmap='Purples',\n",
    "                    annot=True ,fmt=\".2f\", \n",
    "                    annot_kws={'size': 8}, \n",
    "                    vmin=0, \n",
    "                    vmax=1,\n",
    "                    row_cluster=False, \n",
    "                    col_cluster=False, \n",
    "                    figsize=(14, 14))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Reference mapping on Lung-Kim dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be accessed form [here](https://drive.google.com/drive/folders/1gbfO7VqxCOkfzgHAih6hO88zFv6pd8wO?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "adata = sc.read_h5ad('../../data/lung/sample_proc_lung_train.h5ad')\n",
    "test_adata = sc.read_h5ad('../../data/lung/sample_proc_lung_test.h5ad')\n",
    "\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.normalize_total(test_adata, target_sum=1e4)\n",
    "sc.pp.log1p(test_adata)\n",
    "\n",
    "gene_col = \"gene_name\"\n",
    "cell_type_key = \"cell_type\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "source": [
    "ref_embed_adata = scg.tasks.embed_data(\n",
    "    adata,\n",
    "    model_dir,\n",
    "    gene_col=gene_col,\n",
    "    batch_size=64,\n",
    ")\n",
    "test_embed_adata = scg.tasks.embed_data(\n",
    "    test_adata,\n",
    "    model_dir,\n",
    "    gene_col=gene_col,\n",
    "    batch_size=64,\n",
    ")\n",
    "# concatenate the two datasets\n",
    "adata_concat = test_embed_adata.concatenate(ref_embed_adata, batch_key=\"dataset\")\n",
    "# mark the reference vs. query dataset\n",
    "adata_concat.obs[\"is_ref\"] = [\"Query\"] * len(test_embed_adata) + [\"Reference\"] * len(\n",
    "    ref_embed_adata\n",
    ")\n",
    "adata_concat.obs[\"is_ref\"] = adata_concat.obs[\"is_ref\"].astype(\"category\")\n",
    "# mask the query dataset cell types\n",
    "adata_concat.obs[cell_type_key] = adata_concat.obs[cell_type_key].cat.add_categories([\"To be predicted\"])\n",
    "adata_concat.obs[cell_type_key][: len(test_embed_adata)] = \"To be predicted\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "sc.pp.neighbors(adata_concat, use_rep=\"X_scGPT\")\n",
    "sc.tl.umap(adata_concat)\n",
    "sc.pl.umap(\n",
    "    adata_concat, color=[\"is_ref\", cell_type_key], wspace=0.4, frameon=False, ncols=1\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "ref_cell_embeddings = ref_embed_adata.obsm[\"X_scGPT\"]\n",
    "test_emebd = test_embed_adata.obsm[\"X_scGPT\"]\n",
    "\n",
    "k = 10  # number of neighbors\n",
    "\n",
    "index = faiss.IndexFlatL2(ref_cell_embeddings.shape[1])\n",
    "index.add(ref_cell_embeddings)\n",
    "\n",
    "# Query dataset, k - number of closest elements (returns 2 numpy arrays)\n",
    "distances, labels = index.search(test_emebd, k)\n",
    "\n",
    "idx_list=[i for i in range(test_emebd.shape[0])]\n",
    "preds = []\n",
    "sim_list = distances\n",
    "for k in idx_list:\n",
    "    if faiss_imported:\n",
    "        idx = labels[k]\n",
    "    else:\n",
    "        idx, sim = get_similar_vectors(test_emebd[k][np.newaxis, ...], ref_cell_embeddings, k)\n",
    "    pred = ref_embed_adata.obs[cell_type_key][idx].value_counts()\n",
    "    preds.append(pred.index[0])\n",
    "gt = test_adata.obs[cell_type_key].to_numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "res_dict = {\n",
    "    \"accuracy\": accuracy_score(gt, preds),\n",
    "    \"precision\": precision_score(gt, preds, average=\"macro\"),\n",
    "    \"recall\": recall_score(gt, preds, average=\"macro\"),\n",
    "    \"macro_f1\": f1_score(gt, preds, average=\"macro\"),\n",
    "}\n",
    "\n",
    "res_dict"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "source": [
    "y_true = gt\n",
    "y_pred = preds\n",
    "cell_type_list = np.unique(y_true)\n",
    "matrix = confusion_matrix(y_true, y_pred, labels=cell_type_list)\n",
    "matrix = matrix.astype(\"float\") / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "df = pd.DataFrame(matrix, index=cell_type_list[:matrix.shape[0]], columns=cell_type_list[:matrix.shape[1]])\n",
    "\n",
    "ax = sns.clustermap(df,  \n",
    "                    cmap='Purples',\n",
    "                    annot=True ,\n",
    "                    fmt=\".2f\", \n",
    "                    annot_kws={'size': 16}, \n",
    "                    vmin=0, \n",
    "                    vmax=1,\n",
    "                    row_cluster=False, \n",
    "                    col_cluster=False, \n",
    "                    figsize=(14, 14))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
