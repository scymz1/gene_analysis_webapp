{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot integration tutorial with scGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial covers the zero-shot integration with continual pre-trained scGPT. This particular workflow works for scRNA-seq datasets without fine-tuning (or any extensive training) of scGPT.\n",
    "\n",
    "Continual pre-trained scGPT (scGPT_CP) is a model that inherits the pre-trained scGPT whole-human model checkpoint, and is further supervised by extra cell type labels (using the [Tabula Sapiens](https://tabula-sapiens-portal.ds.czbiohub.org/) dataset) during the continual pre-training stage. We observed that the scGPT_CP model can achieve comparable or better zero-shot performance on cell embedding related tasks compared to the original checkpoint, especially on datasets with observable technical batch effects.\n",
    "\n",
    "This tutorial will show how to use the latent space of scGPT to integrate scRNA-seq datasets. We use the `scGPT_CP` model to provide embeddings out of the box. You may download it from [here](https://drive.google.com/drive/folders/1_GROJTzXiAV8HB4imruOTk6PEGuNOcgB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [scIB](https://www.nature.com/articles/s41592-021-01336-8) pancreas dataset as an example. This dataset is publicly accessible via [here](https://figshare.com/ndownloader/files/24539828). You may place the dataset under `data` directory at the outer level.\n",
    "\n",
    "\n",
    "The zero-shot integration workflow is as follows:\n",
    "\n",
    " 1. [Load and pre-process the dataset](#prepare-the-datasets)\n",
    "    \n",
    " 2. [Generate scGPT embeddings for each cell](#generate-the-cell-embeddings)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bowang-lab/scGPT/blob/main/tutorials/zero-shot/Tutorial_ZeroShot_Integration_Continual_Pretraining.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# Specifically for Google Colab, install dependencies and download data\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    print(\"Installing dependencies...\")\n",
    "    !pip install -U scgpt\n",
    "    # the optional dependency of flash-attion is skipped on colab\n",
    "    !pip install wandb louvain\n",
    "\n",
    "    # NOTE: May need to restart runtime after the installation\n",
    "\n",
    "    print(\"Downloading data and model ckpt...\")\n",
    "    !pip install -q -U gdown\n",
    "    import gdown\n",
    "\n",
    "    data_dir = \"../../data\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "    if not os.path.exists(os.path.join(data_dir, \"human_pancreas_norm_complexBatch.h5ad\")):\n",
    "        !wget --content-disposition https://figshare.com/ndownloader/files/24539828 -O $data_dir/human_pancreas_norm_complexBatch.h5ad\n",
    "\n",
    "    print(\"Downloading model ckpt...\")\n",
    "    model_dir = \"../../save/scGPT_CP\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        !mkdir -p $model_dir\n",
    "        gdown.download_folder(\n",
    "            \"https://drive.google.com/drive/folders/1_GROJTzXiAV8HB4imruOTk6PEGuNOcgB?usp=sharing\",\n",
    "            output=model_dir,\n",
    "        )\n",
    "    model_dir = \"../../save/scGPT_human\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        !mkdir -p $model_dir\n",
    "        gdown.download_folder(\n",
    "            \"https://drive.google.com/drive/folders/1oWh_-ZRdhtoGQ2Fw24HP41FgLoomVo-y\",\n",
    "            output=model_dir,\n",
    "        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import scGPT and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import scanpy as sc\n",
    "import scib\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import scgpt as scg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.context('default')\n",
    "warnings.simplefilter(\"ignore\", ResourceWarning)\n",
    "\n",
    "model_dir = Path(\"../../save/scGPT_CP\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the evaluation function, we mainly compare the integration performance on avgBIO and avgBATCH. Refer to our manuscript for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Calculate the metrics for integration results\n",
    "\"\"\"\n",
    "def scib_eval(adata, batch_key, cell_type_key, embed_key):\n",
    "    results = scib.metrics.metrics(\n",
    "        adata,\n",
    "        adata_int=adata,\n",
    "        batch_key=batch_key,\n",
    "        label_key=cell_type_key,\n",
    "        embed=embed_key,\n",
    "        isolated_labels_asw_=False,\n",
    "        silhouette_=True,\n",
    "        hvg_score_=False,\n",
    "        graph_conn_=True,\n",
    "        pcr_=True,\n",
    "        isolated_labels_f1_=False,\n",
    "        trajectory_=False,\n",
    "        nmi_=True,  # use the clustering, bias to the best matching\n",
    "        ari_=True,  # use the clustering, bias to the best matching\n",
    "        cell_cycle_=False,\n",
    "        kBET_=False,  # kBET return nan sometimes, need to examine\n",
    "        ilisi_=False,\n",
    "        clisi_=False,\n",
    "    )\n",
    "    result_dict = results[0].to_dict()\n",
    "    \n",
    "    # compute avgBIO metrics\n",
    "    result_dict[\"avg_bio\"] = np.mean(\n",
    "        [\n",
    "            result_dict[\"NMI_cluster/label\"],\n",
    "            result_dict[\"ARI_cluster/label\"],\n",
    "            result_dict[\"ASW_label\"],\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # compute avgBATCH metrics\n",
    "    result_dict[\"avg_batch\"] = np.mean(\n",
    "        [\n",
    "            result_dict[\"graph_conn\"],\n",
    "            result_dict[\"ASW_label/batch\"],\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    result_dict = {k: v for k, v in result_dict.items() if not np.isnan(v)}\n",
    "    \n",
    "    return result_dict"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Pancreas dataset (download it from [here](https://figshare.com/ndownloader/files/24539828)), and we set the columns storing gene name columns, batch key and cell type key (optional, this is for evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "smaple_data_path = '../../data/human_pancreas_norm_complexBatch.h5ad'\n",
    "adata = sc.read_h5ad(smaple_data_path)\n",
    "\n",
    "gene_col = \"Gene Symbol\"\n",
    "cell_type_key = \"celltype\"\n",
    "batch_key = \"tech\"\n",
    "N_HVG = 1800"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "adata.var[gene_col] = adata.var.index.values"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "org_adata = adata.copy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the dataset and select `N_HVG` highly variable genes for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# preprocess\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "# highly variable genes\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=N_HVG, flavor='seurat_v3')\n",
    "adata = adata[:, adata.var['highly_variable']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the cell embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate the cell embeddings for the dataset using `embed_data` function. `embed_data` calculates the cell embedding for each cell with the given scGPT model. The extracted embedding is stored in the `X_scGPT` field of `obsm` in AnnData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "embed_adata = scg.tasks.embed_data(\n",
    "    adata,\n",
    "    model_dir,\n",
    "    gene_col=gene_col,\n",
    "    batch_size=64,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the integration performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP of scGPT embedding colored by cell type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "sc.pp.neighbors(embed_adata, use_rep=\"X_scGPT\")\n",
    "sc.tl.umap(embed_adata)\n",
    "sc.pl.umap(embed_adata, \n",
    "           color=[cell_type_key, batch_key], \n",
    "           frameon=False, \n",
    "           wspace=0.4, \n",
    "           title=[\"scGPT zero-shot: cell type\", \"scGPT zero-shot: batch label\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the integration performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "scib_result_dict = scib_eval(\n",
    "    embed_adata,\n",
    "    batch_key=batch_key,\n",
    "    cell_type_key=cell_type_key,\n",
    "    embed_key=\"X_scGPT\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "print(\"AvgBIO: {:.4f}\".format(scib_result_dict[\"avg_bio\"]))\n",
    "print(\"AvgBATCH: {:.4f}\".format(scib_result_dict[\"avg_batch\"]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with the original scGPT model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "embed_adata = scg.tasks.embed_data(\n",
    "    adata,\n",
    "    \"../../save/scGPT_human\",\n",
    "    gene_col=gene_col,\n",
    "    batch_size=64,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "sc.pp.neighbors(embed_adata, use_rep=\"X_scGPT\")\n",
    "sc.tl.umap(embed_adata)\n",
    "sc.pl.umap(embed_adata, \n",
    "           color=[cell_type_key, batch_key], \n",
    "           frameon=False, \n",
    "           wspace=0.4, \n",
    "           title=[\"scGPT zero-shot: cell type\", \"scGPT zero-shot: batch label\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "scib_result_dict = scib_eval(\n",
    "    embed_adata,\n",
    "    batch_key=batch_key,\n",
    "    cell_type_key=cell_type_key,\n",
    "    embed_key=\"X_scGPT\",\n",
    ")\n",
    "print(\"AvgBIO: {:.4f}\".format(scib_result_dict[\"avg_bio\"]))\n",
    "print(\"AvgBATCH: {:.4f}\".format(scib_result_dict[\"avg_batch\"]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with HVG+PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the HVG and PCs based on the dataset, this will serve as a baseline for the integration performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "adata = org_adata.copy()\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=N_HVG, flavor='seurat_v3')\n",
    "adata = adata[:, adata.var['highly_variable']]\n",
    "sc.pp.pca(adata, n_comps=40)\n",
    "sc.pp.neighbors(adata, use_rep=\"X_pca\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP of PCs colored by cell types and batch labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "sc.pp.neighbors(adata, use_rep=\"X_pca\")\n",
    "sc.tl.umap(adata)\n",
    "sc.pl.umap(adata, \n",
    "           color=[cell_type_key, batch_key], \n",
    "           frameon=False, \n",
    "           wspace=0.4, \n",
    "           title=[\"HVG+PCs: cell type\", \"HVG+PCs: batch label\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "scib_result_dict = scib_eval(\n",
    "    adata,\n",
    "    batch_key=batch_key,\n",
    "    cell_type_key=cell_type_key,\n",
    "    embed_key=\"X_pca\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "print(\"AvgBIO: {:.4f}\".format(scib_result_dict[\"avg_bio\"]))\n",
    "print(\"AvgBATCH: {:.4f}\".format(scib_result_dict[\"avg_batch\"]))"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt-o4JycL9C-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
