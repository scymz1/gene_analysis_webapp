{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbebfa3",
   "metadata": {},
   "source": [
    "# Attention-based GRN Inference on Fine-tuned Model\n",
    "Here we use the fine-tuned blood model on the Adamson perturbation dataset as an example of the cell-state specific GRN inference via attention weights. scGPT outputs attention weights on the individual cell level, which can be further aggregated by cell states. In this particular example, we compare the most influenced genes between a transcription factor repression condition (perturbed) and the control. However, this attention-based GRN inference is not restricted to perturbation-based discoveries. It can also be used to compare between cell states in general, such as healthy v.s. diseased, undifferentiated v.s. differentiated, as a broader application.\n",
    "\n",
    "Users may perform scGPT's attention-based GRN inference in the following steps:\n",
    "\n",
    "     1. Load fine-tuned scGPT model and data\n",
    "     \n",
    "     2. Retrieve scGPT's attention weights by condition (i.e., cell states)\n",
    "     \n",
    "     3. Perform scGPT's rank-based most influenced gene selection\n",
    "     \n",
    "     4. Validate the most influenced gene list against existing databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b674f4",
   "metadata": {},
   "source": [
    "NOTE in advance: to run this tutorial notebook, you may need to download the fine-tuned model from [link](https://drive.google.com/drive/folders/1HsPrwYGPXm867_u_Ye0W4Ch8AFSneXAn) and the list of targets of BHLHE40 from CHIP-Atlas for evaluation from [link](https://drive.google.com/drive/folders/1nc1LywRHlzt4Z17McfXiqBWgoGbRNyc0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a59dea6",
   "metadata": {},
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import gseapy as gp\n",
    "from gears import PertData, GEARS\n",
    "\n",
    "from scipy.sparse import issparse\n",
    "import scipy as sp\n",
    "from einops import rearrange\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import scgpt as scg\n",
    "from scgpt.tasks import GeneEmbedding\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.utils import set_seed \n",
    "from scgpt.tokenizer import tokenize_and_pad_batch\n",
    "from scgpt.preprocess import Preprocessor\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d846b2e",
   "metadata": {},
   "source": [
    "set_seed(42)\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "n_hvg = 1200\n",
    "n_bins = 51\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = n_bins"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2c246caf",
   "metadata": {},
   "source": [
    "## Step 1: Load fine-tuned model and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e73800",
   "metadata": {},
   "source": [
    "### 1.1  Load fine-tuned model\n",
    "\n",
    "We are going to load a fine-tuned model for the gene interaction analysis on Adamson dataset. The fine-tuned model can be downloaded via this [link](https://drive.google.com/drive/folders/1HsPrwYGPXm867_u_Ye0W4Ch8AFSneXAn). The dataset will be loaded in the next step 1.2.\n",
    "\n",
    "To reproduce the provided fine-tuned model. Please followw the integration fin-tuning pipeline to fine-tune the pre-trained blood model on the Adamson perturbation dataset. Note that in the fine-tuning stage, we did not perform highly vairable gene selection but trained on the 5000+ genes present in the Adamson dataset. This is to provide flexbility in the inference stage to investigate changes in attention maps across different perturbation conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a45d1c9",
   "metadata": {},
   "source": [
    "# Specify model path; here we load the scGPT blood model fine-tuned on adamson\n",
    "model_dir = Path(\"../save/finetuned_scGPT_adamson\")\n",
    "model_config_file = model_dir / \"args.json\"\n",
    "model_file = model_dir / \"best_model.pt\"\n",
    "vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "for s in special_tokens:\n",
    "    if s not in vocab:\n",
    "        vocab.append_token(s)\n",
    "\n",
    "# Retrieve model parameters from config files\n",
    "with open(model_config_file, \"r\") as f:\n",
    "    model_configs = json.load(f)\n",
    "print(\n",
    "    f\"Resume model from {model_file}, the model args will override the \"\n",
    "    f\"config {model_config_file}.\"\n",
    ")\n",
    "embsize = model_configs[\"embsize\"]\n",
    "nhead = model_configs[\"nheads\"]\n",
    "d_hid = model_configs[\"d_hid\"]\n",
    "nlayers = model_configs[\"nlayers\"]\n",
    "n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "gene2idx = vocab.get_stoi()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed785b4",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    pad_value=pad_value,\n",
    "    n_input_bins=n_input_bins,\n",
    "    use_fast_transformer=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    print(f\"Loading all model params from {model_file}\")\n",
    "except:\n",
    "    # only load params that are in the model and match the size\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(model_file)\n",
    "    pretrained_dict = {\n",
    "        k: v\n",
    "        for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    for k, v in pretrained_dict.items():\n",
    "        print(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ebdda8f4",
   "metadata": {},
   "source": [
    "### 1.2  Load dataset of interest\n",
    "The Adamson perturbation dataset is retrieved from the GEARS package with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3c1d15",
   "metadata": {},
   "source": [
    "data_dir = Path(\"../data\")\n",
    "pert_data = PertData(data_dir)\n",
    "pert_data.load(data_name=\"adamson\")\n",
    "adata = sc.read(data_dir / \"adamson/perturb_processed.h5ad\")\n",
    "ori_batch_col = \"control\"\n",
    "adata.obs[\"celltype\"] = adata.obs[\"condition\"].astype(\"category\")\n",
    "adata.obs[\"str_batch\"] = adata.obs[\"control\"].astype(str)\n",
    "data_is_raw = False"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0feb668",
   "metadata": {},
   "source": [
    "adata.var[\"id_in_vocab\"] = [1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]]\n",
    "gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eda94662",
   "metadata": {},
   "source": [
    "In the scGPT workflow, we compare each TF perturbation condition with control one at a time. In each run, the data is subsetted to contain one TF and control only. In this example, we use the TF BHLHE40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d99c7d82",
   "metadata": {},
   "source": [
    "TF_name = 'BHLHE40'\n",
    "adata = adata[adata.obs.condition.isin(['{}+ctrl'.format(TF_name), 'ctrl'])].copy()\n",
    "np.unique(adata.obs.condition)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e5a8d08c",
   "metadata": {},
   "source": [
    "We further pre-process the subsetted data following the scGPT pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b81799b8",
   "metadata": {},
   "source": [
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg= False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=\"str_batch\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da616114",
   "metadata": {},
   "source": [
    "sc.pp.highly_variable_genes(\n",
    "    adata,\n",
    "    layer=None,\n",
    "    n_top_genes=1200,\n",
    "    batch_key=\"str_batch\",\n",
    "    flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    subset=False,\n",
    ")\n",
    "adata.var.loc[adata.var[adata.var.gene_name==TF_name].index, 'highly_variable'] = True\n",
    "adata = adata[:, adata.var[\"highly_variable\"]].copy()\n",
    "print(adata)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd3ffd5",
   "metadata": {},
   "source": [
    "## Step 2: Retrieve scGPT's attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc26b6",
   "metadata": {},
   "source": [
    "### 2.1 Prepare model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5639266",
   "metadata": {},
   "source": [
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83bc0468",
   "metadata": {},
   "source": [
    "batch_size = 16\n",
    "tokenized_all = tokenize_and_pad_batch(\n",
    "    all_counts,\n",
    "    gene_ids,\n",
    "    max_len=len(genes)+1,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=True,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4bea9d",
   "metadata": {},
   "source": [
    "all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "condition_ids = np.array(adata.obs[\"condition\"].tolist())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f6661cd8",
   "metadata": {},
   "source": [
    "### 2.1 Retrieve attention weights\n",
    "Note that since the flash-attn package does not output attention scores, we manually calculate q @ k.T to extract the attention weights. Users may specify which layer to extract the attention weights from. In the manuscript, we used the attention weights from the last (12th) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb6993a7",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "dict_sum_condition = {}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91bd96f8",
   "metadata": {},
   "source": [
    "# Use this argument to specify which layer to extract the attention weights from\n",
    "# Default to 11, extraction from the last (12th) layer. Note that index starts from 0\n",
    "num_attn_layers = 11 "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53301b87",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(enabled=True):\n",
    "    M = all_gene_ids.size(1)\n",
    "    N = all_gene_ids.size(0)\n",
    "    device = next(model.parameters()).device\n",
    "    for i in tqdm(range(0, N, batch_size)):\n",
    "        batch_size = all_gene_ids[i : i + batch_size].size(0)\n",
    "        outputs = np.zeros((batch_size, M, M), dtype=np.float32)\n",
    "        # Replicate the operations in model forward pass\n",
    "        src_embs = model.encoder(torch.tensor(all_gene_ids[i : i + batch_size], dtype=torch.long).to(device))\n",
    "        val_embs = model.value_encoder(torch.tensor(all_values[i : i + batch_size], dtype=torch.float).to(device))\n",
    "        total_embs = src_embs + val_embs\n",
    "        total_embs = model.bn(total_embs.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        # Send total_embs to attention layers for attention operations\n",
    "        # Retrieve the output from second to last layer\n",
    "        for layer in model.transformer_encoder.layers[:num_attn_layers]:\n",
    "            total_embs = layer(total_embs, src_key_padding_mask=src_key_padding_mask[i : i + batch_size].to(device))\n",
    "        # Send total_embs to the last layer in flash-attn\n",
    "        # https://github.com/HazyResearch/flash-attention/blob/1b18f1b7a133c20904c096b8b222a0916e1b3d37/flash_attn/flash_attention.py#L90\n",
    "        qkv = model.transformer_encoder.layers[num_attn_layers].self_attn.Wqkv(total_embs)\n",
    "        # Retrieve q, k, and v from flast-attn wrapper\n",
    "        qkv = rearrange(qkv, 'b s (three h d) -> b s three h d', three=3, h=8)\n",
    "        q = qkv[:, :, 0, :, :]\n",
    "        k = qkv[:, :, 1, :, :]\n",
    "        v = qkv[:, :, 2, :, :]\n",
    "        # https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n",
    "        # q = [batch, gene, n_heads, n_hid]\n",
    "        # k = [batch, gene, n_heads, n_hid]\n",
    "        # attn_scores = [batch, n_heads, gene, gene]\n",
    "        attn_scores = q.permute(0, 2, 1, 3) @ k.permute(0, 2, 3, 1)\n",
    "        # Rank normalization by row\n",
    "        attn_scores = attn_scores.reshape((-1, M))\n",
    "        order = torch.argsort(attn_scores, dim=1)\n",
    "        rank = torch.argsort(order, dim=1)\n",
    "        attn_scores = rank.reshape((-1, 8, M, M))/M\n",
    "        # Rank normalization by column\n",
    "        attn_scores = attn_scores.permute(0, 1, 3, 2).reshape((-1, M))\n",
    "        order = torch.argsort(attn_scores, dim=1)\n",
    "        rank = torch.argsort(order, dim=1)\n",
    "        attn_scores = (rank.reshape((-1, 8, M, M))/M).permute(0, 1, 3, 2)\n",
    "        # Average 8 attention heads\n",
    "        attn_scores = attn_scores.mean(1)\n",
    "        \n",
    "        outputs = attn_scores.detach().cpu().numpy()\n",
    "        \n",
    "        for index in range(batch_size):\n",
    "            # Keep track of sum per condition\n",
    "            c = condition_ids[i : i + batch_size][index]\n",
    "            if c not in dict_sum_condition:\n",
    "                dict_sum_condition[c] = np.zeros((M, M), dtype=np.float32)\n",
    "            else:\n",
    "                dict_sum_condition[c] += outputs[index, :, :]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "571d16b4",
   "metadata": {},
   "source": [
    "### 2.2 Average rank-normed attention weights by condition\n",
    "In the previous step, we retrieve the attention weights for all cells and keep the running sum by condition (i.e., control, perturbed). We further calculate the mean here by dividing the number of cells per condition to obtain a gene * gene attention matrix for each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2544f5dc",
   "metadata": {},
   "source": [
    "groups = adata.obs.groupby('condition').groups"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8485f055",
   "metadata": {},
   "source": [
    "dict_sum_condition_mean = dict_sum_condition.copy()\n",
    "for i in groups.keys():\n",
    "    dict_sum_condition_mean[i] = dict_sum_condition_mean[i]/len(groups[i])\n",
    "gene_vocab_idx = all_gene_ids[0].clone().detach().cpu().numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b23f521",
   "metadata": {},
   "source": [
    "dict_sum_condition_mean"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "066e8d9a",
   "metadata": {},
   "source": [
    "## Step 3: Perform most influenced gene selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89dbcf",
   "metadata": {},
   "source": [
    "In the manuscript, we proposed 3 settings for the most influenced gene selection, namely *Control*, *Perturb*, and *Difference*. In this example, we focus on the *Difference* setting to explore how the gene-gene network changes after perturbation compared to control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9c4abeb",
   "metadata": {},
   "source": [
    "def get_topk_most_influenced_genes(topk, setting):\n",
    "    attn_top_gene_dict = {}\n",
    "    attn_top_scores_dict = {}\n",
    "    for i in groups.keys():\n",
    "        if i != 'ctrl':\n",
    "            knockout_gene = i.split('+')[0]\n",
    "            knockout_gene_idx = np.where(gene_vocab_idx==vocab([knockout_gene])[0])[0][0]\n",
    "            control = dict_sum_condition_mean['ctrl'][:, knockout_gene_idx]\n",
    "            exp = dict_sum_condition_mean[i][:, knockout_gene_idx]\n",
    "            # Chnage this line to exp, control, exp-control for three different settings\n",
    "            if setting == 'difference':\n",
    "                a = exp-control\n",
    "            elif setting == 'control':\n",
    "                a = control\n",
    "            elif setting == 'experiment':\n",
    "                a = exp\n",
    "            diff_idx = np.argpartition(a, -topk)[-topk:]\n",
    "            scores = (a)[diff_idx]\n",
    "            attn_top_genes = vocab.lookup_tokens(gene_vocab_idx[diff_idx]) + [TF_name]\n",
    "            attn_top_gene_dict[i] = list(attn_top_genes)\n",
    "            attn_top_scores_dict[i] = list(scores)\n",
    "    return attn_top_gene_dict, attn_top_scores_dict"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cceab4f4",
   "metadata": {},
   "source": [
    "gene_vocab_idx = all_gene_ids[0].clone().detach().cpu().numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd2a086b",
   "metadata": {},
   "source": [
    "# Specify top k number of genes to be selected, and the selection setting\n",
    "# Here calculate top 20 most influenced genes for CHIP-Atlas validation\n",
    "topk = 20\n",
    "setting = 'difference' # \"control\", \"perturbed\"\n",
    "assert setting in [\"difference\", \"control\", \"perturbed\"]\n",
    "attn_top_gene_dict_20, attn_top_scores_dict_20 = get_topk_most_influenced_genes(topk, setting)\n",
    "print(attn_top_scores_dict_20[TF_name + '+ctrl'])\n",
    "print(attn_top_gene_dict_20[TF_name + '+ctrl'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "802bcd2c",
   "metadata": {},
   "source": [
    "if setting == 'difference':\n",
    "    for i in attn_top_gene_dict_20.keys():\n",
    "        example_genes = attn_top_gene_dict_20[i]\n",
    "        gene_idx = [np.where(gene_vocab_idx==vocab([g])[0])[0][0] for g in example_genes]\n",
    "        scores = dict_sum_condition_mean[i][gene_idx, :][:, gene_idx]-dict_sum_condition_mean['ctrl'][gene_idx, :][:, gene_idx]\n",
    "        df_scores = pd.DataFrame(data = scores, columns = example_genes, index = example_genes)\n",
    "        plt.figure(figsize=(6, 6), dpi=300)\n",
    "        ax = sns.clustermap(df_scores, annot=False, cmap=sns.diverging_palette(145, 300, s=60, as_cmap=True), fmt='.2f', vmin=-0.3, vmax=0.3) \n",
    "        plt.show()\n",
    "        plt.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b0693fa",
   "metadata": {},
   "source": [
    "# Specify top k number of genes to be selected, and the selection setting\n",
    "# # Here calculate top 100 most influenced genes for pathway validation\n",
    "topk = 100\n",
    "setting = 'difference' # \"control\", \"perturbed\"\n",
    "assert setting in [\"difference\", \"control\", \"perturbed\"]\n",
    "attn_top_gene_dict_100, attn_top_scores_dict_100 = get_topk_most_influenced_genes(topk, setting)\n",
    "print(attn_top_scores_dict_100[TF_name + '+ctrl'])\n",
    "print(attn_top_gene_dict_100[TF_name + '+ctrl'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "db5083ac",
   "metadata": {},
   "source": [
    "## Step 4: Validate most influenced genes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025cde91",
   "metadata": {},
   "source": [
    "### Step 4.1: Validate against CHIP-Atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e8242",
   "metadata": {},
   "source": [
    "First load the tsv file from CHIP-Atlas containing targets of BHLHE40. The tsv file for BHLHE40 can be downloaded via this [link](https://drive.google.com/drive/folders/1nc1LywRHlzt4Z17McfXiqBWgoGbRNyc0). This tsv file was originally retrieved from the [CHIP-Atlas](https://chip-atlas.org/target_genes) website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3089cb62",
   "metadata": {},
   "source": [
    "df = pd.read_csv('./reference/BHLHE40.10.tsv', delimiter='\\t')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8fa4ada4",
   "metadata": {},
   "source": [
    "Examine the overalp between the selected genes (top 20) and known target genes from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcd587ec",
   "metadata": {},
   "source": [
    "gene_list = attn_top_gene_dict_20[TF_name + '+ctrl'][:-1]\n",
    "set(gene_list).intersection(set(df['Target_genes'].values)), len(set(gene_list).intersection(set(df['Target_genes'].values)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "50001e4f",
   "metadata": {},
   "source": [
    "Visualize the network and strength of the edges (annotated with rank-normalized attention scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9752a485",
   "metadata": {},
   "source": [
    "score_list = attn_top_scores_dict_20[TF_name + '+ctrl']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30807b1b",
   "metadata": {},
   "source": [
    "hits = set(gene_list).intersection(set(df['Target_genes'].values))\n",
    "\n",
    "G = nx.DiGraph()\n",
    "edge_list = [(TF_name, gene_list[i], round(score_list[i], 2)) for i in range(len(gene_list))]\n",
    "G.add_weighted_edges_from(edge_list)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "edges = list(G.edges)\n",
    "elarge = [(u, v) for (u, v, d) in G.edges(data=True) if v in hits]\n",
    "esmall = [(u, v) for (u, v, d) in G.edges(data=True) if v not in hits]\n",
    "pos = nx.shell_layout(G)\n",
    "width_large = {}\n",
    "width_small = {}\n",
    "for i, v in enumerate(edges):\n",
    "    if v[1] in hits:\n",
    "        width_large[edges[i]] = G.get_edge_data(v[0], v[1])['weight']*30\n",
    "    else:\n",
    "        width_small[edges[i]] = max(G.get_edge_data(v[0], v[1])['weight'], 0)*30\n",
    "nx.draw_networkx_edges(G, pos,\n",
    "                       edgelist = width_small.keys(),\n",
    "                       width=list(width_small.values()),\n",
    "                       edge_color='grey',\n",
    "                       alpha=0.8)\n",
    "nx.draw_networkx_edges(G, pos, \n",
    "                       edgelist = width_large.keys(), \n",
    "                       width = list(width_large.values()), \n",
    "                       alpha = 0.6, \n",
    "                       edge_color = \"slateblue\",\n",
    "                      )\n",
    "labels = {}\n",
    "for i in pos.keys():\n",
    "    if i == TF_name:\n",
    "        labels[i] = ''\n",
    "    else:\n",
    "        labels[i] = i\n",
    "        \n",
    "labels1 = {}\n",
    "for i in pos.keys():\n",
    "    if i != TF_name:\n",
    "        labels1[i] = ''\n",
    "    else:\n",
    "        labels1[i] = i\n",
    "nx.draw_networkx_labels(G, pos, labels, font_size=30, font_family=\"sans-serif\", horizontalalignment='right')\n",
    "nx.draw_networkx_labels(G, pos, labels1, font_size=30, font_family=\"sans-serif\", font_weight='bold', horizontalalignment='right')\n",
    "\n",
    "d = nx.get_edge_attributes(G, \"weight\")\n",
    "edge_labels = {k: d[k] for k in elarge}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=20)\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1c006c98",
   "metadata": {},
   "source": [
    "### Step 4.2: Validate against the Reactome database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686dcf3",
   "metadata": {},
   "source": [
    "We perform pathway analysis on the top 100 most influenced genes by checking against the Reactome database. This replicates the reported pathways found in the *Difference* setting in the manuscript for the select TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05ded107",
   "metadata": {},
   "source": [
    "# Validate with Reactome \n",
    "df_database = pd.DataFrame(\n",
    "data = [['GO_Biological_Process_2021', 6036],\n",
    "['GO_Molecular_Function_2021', 1274],\n",
    "['Reactome_2022', 1818]],\n",
    "columns = ['dataset', 'term'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a01e550d",
   "metadata": {},
   "source": [
    "databases = ['Reactome_2022']\n",
    "m = df_database[df_database['dataset'].isin(databases)]['term'].sum() #df_database['term'].sum()\n",
    "p_thresh = 0.05/((len(groups.keys())-1)*m)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8de3374a",
   "metadata": {},
   "source": [
    "gene_list = attn_top_gene_dict_100[TF_name + '+ctrl']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acd997e1",
   "metadata": {},
   "source": [
    "df_attn = pd.DataFrame()\n",
    "enr_Reactome = gp.enrichr(gene_list=gene_list,\n",
    "                           gene_sets=databases,\n",
    "                           organism='Human', \n",
    "                           outdir='test',\n",
    "                           cutoff=0.5)\n",
    "out = enr_Reactome.results\n",
    "out['Gene List'] = str(gene_list)\n",
    "out = out[out['P-value'] < p_thresh]\n",
    "df_attn = df_attn.append(out, ignore_index=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b556f31",
   "metadata": {},
   "source": [
    "len(df_attn)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ea5a0ca",
   "metadata": {},
   "source": [
    "df_attn"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
