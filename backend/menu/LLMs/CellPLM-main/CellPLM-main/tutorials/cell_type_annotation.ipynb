{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56fc5a70",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aafcd9dc",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import hdf5plugin\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "from scipy.sparse import csr_matrix\n",
    "from CellPLM.utils import set_seed\n",
    "from CellPLM.pipeline.cell_type_annotation import CellTypeAnnotationPipeline, CellTypeAnnotationDefaultPipelineConfig, CellTypeAnnotationDefaultModelConfig"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "536d3952",
   "metadata": {},
   "source": [
    "## Specify important parameters before getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874f0469",
   "metadata": {},
   "source": [
    "DATASET = 'MS' # 'hPancreas'\n",
    "PRETRAIN_VERSION = '20230926_85M'\n",
    "DEVICE = 'cuda:3'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "044a88ad",
   "metadata": {},
   "source": [
    "## Load Downstream Dataset\n",
    "\n",
    "The MS dataset is contributed by [scGPT](https://github.com/bowang-lab/scGPT/blob/main/tutorials/Tutorial_Annotation.ipynb). hPancreas dataset is contributed by [TOSICA](https://github.com/JackieHanLab/TOSICA/blob/main/test/tutorial.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c4d4d6",
   "metadata": {},
   "source": [
    "set_seed(42)\n",
    "if DATASET == 'hPancreas':\n",
    "    data_train = ad.read_h5ad(f'../data/demo_train.h5ad')\n",
    "    data_test = ad.read_h5ad(f'../data/demo_test.h5ad')\n",
    "    train_num = data_train.shape[0]\n",
    "    data = ad.concat([data_train, data_test])\n",
    "    data.X = csr_matrix(data.X)\n",
    "    data.obs['celltype'] = data.obs['Celltype']\n",
    "\n",
    "elif DATASET == 'MS':\n",
    "    data_train = ad.read_h5ad(f'../data/c_data.h5ad')\n",
    "    data_test = ad.read_h5ad(f'../data/filtered_ms_adata.h5ad')\n",
    "    data_train.var = data_train.var.set_index('index_column')\n",
    "    data_test.var = data_test.var.set_index('index_column')\n",
    "    train_num = data_train.shape[0]\n",
    "    data = ad.concat([data_train, data_test])\n",
    "    data.var_names_make_unique()\n",
    "\n",
    "data.obs['split'] = 'test'\n",
    "tr = np.random.permutation(train_num) #torch.randperm(train_num).numpy()\n",
    "data.obs['split'][tr[:int(train_num*0.9)]] = 'train'\n",
    "data.obs['split'][tr[int(train_num*0.9):train_num]] = 'valid'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ff96bc99",
   "metadata": {},
   "source": [
    "## Overwrite parts of the default config\n",
    "These hyperparameters are recommended for general purpose. We did not tune it for individual datasets. You may update them if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c6cccb6",
   "metadata": {},
   "source": [
    "pipeline_config = CellTypeAnnotationDefaultPipelineConfig.copy()\n",
    "\n",
    "model_config = CellTypeAnnotationDefaultModelConfig.copy()\n",
    "model_config['out_dim'] = data.obs['celltype'].nunique()\n",
    "pipeline_config, model_config"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5dfe80a4",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd48872",
   "metadata": {},
   "source": [
    "Efficient data setup and fine-tuning can be seamlessly conducted using the CellPLM built-in `pipeline` module.\n",
    "\n",
    "First, initialize a `CellTypeAnnotationPipeline`. This pipeline will automatically load a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54b753e",
   "metadata": {},
   "source": [
    "pipeline = CellTypeAnnotationPipeline(pretrain_prefix=PRETRAIN_VERSION, # Specify the pretrain checkpoint to load\n",
    "                                      overwrite_config=model_config,  # This is for overwriting part of the pretrain config\n",
    "                                      pretrain_directory='../ckpt')\n",
    "pipeline.model"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "12d7d4a9",
   "metadata": {},
   "source": [
    "Next, employ the `fit` function to fine-tune the model on your downstream dataset. This dataset should be in the form of an AnnData object, where `.X` is a csr_matrix, and `.obs` includes information for train-test splitting and cell type labels.\n",
    "\n",
    "Typically, a dataset containing approximately 20,000 cells can be trained in under 10 minutes using a V100 GPU card, with an expected GPU memory consumption of around 8GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b0b9cb",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pipeline.fit(data, # An AnnData object\n",
    "            pipeline_config, # The config dictionary we created previously, optional\n",
    "            split_field = 'split', #  Specify a column in .obs that contains split information\n",
    "            train_split = 'train',\n",
    "            valid_split = 'valid',\n",
    "            label_fields = ['celltype']) # Specify a column in .obs that contains cell type labels"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "172f80ee",
   "metadata": {},
   "source": [
    "## Inference and evaluation\n",
    "Once the pipeline has been fitted to the downstream datasets, performing inference or evaluation on new datasets can be easily accomplished using the built-in `predict` and `score` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99af1706",
   "metadata": {},
   "source": [
    "pipeline.predict(\n",
    "                data, # An AnnData object\n",
    "                pipeline_config, # The config dictionary we created previously, optional\n",
    "            )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f773fa50",
   "metadata": {},
   "source": [
    "pipeline.score(data, # An AnnData object\n",
    "                pipeline_config, # The config dictionary we created previously, optional\n",
    "                split_field = 'split', # Specify a column in .obs to specify train and valid split, optional\n",
    "                target_split = 'test', # Specify a target split to predict, optional\n",
    "                label_fields = ['celltype'])  # Specify a column in .obs that contains cell type labels"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62de0f9",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellplm",
   "language": "python",
   "name": "cellplm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
